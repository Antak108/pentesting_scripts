import requests
import re
import beautifulsoup4

# Set the starting URL
url = 'https://www.example.com/'

# Set the maximum number of pages to crawl
max_pages = 10

# Set the regular expression pattern to match domain names
pattern = re.compile(r'(?:https?://)?(?:www\.)?([^/]+)')

# Initialize a list to store the domain names
domain_names = []

# Crawl the website
for page in range(max_pages):
  # Send a GET request to the URL
  response = requests.get(url)

  # Check the response status code
  if response.status_code != 200:
    print(f'Error: {response.status_code}')
    break

  # Extract the domain names from the response
  matches = pattern.finditer(response.text)
  for match in matches:
    domain_name = match.group(1)
    domain_names.append(domain_name)

  # Get the next URL to crawl
  soup = beautifulsoup4.BeautifulSoup(response.text, 'html.parser')
  next_link = soup.find('a', {'class': 'next-link'})
  if next_link:
    url = next_link['href']
  else:
    break

# Write the domain names to a text file
with open('domain_names.txt', 'w') as f:
  for domain
  